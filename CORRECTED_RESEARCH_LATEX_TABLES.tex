% CORRECTED LaTeX Tables for EGE Mathematics Benchmark Research Paper
% Generated from comprehensive evaluation results - ALL ERRORS FIXED

\documentclass{article}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{rotating}

\begin{document}

% CORRECTED Dataset Characteristics Table
\begin{table}[htbp]
\centering
\caption{EGE Mathematics Dataset Characteristics (CORRECTED)}
\label{tab:dataset_characteristics_corrected}
\begin{tabular}{@{}clcc@{}}
\toprule
\textbf{Task ID} & \textbf{Mathematical Domain} & \textbf{Count} & \textbf{Score Range} \\
\midrule
13 & Trigonometry & 21 & 0-2 points \\
14 & Stereometry & 18 & \textbf{0-3 points} \\
15 & Inequalities & 19 & 0-2 points \\
16 & Planimetry & 17 & \textbf{0-3 points} \\
17 & Financial Mathematics & 15 & \textbf{0-3 points} \\
18 & Parametric Problems & 16 & \textbf{0-4 points} \\
19 & Number Theory & 16 & \textbf{0-4 points} \\
\midrule
\textbf{Total} & & \textbf{122} & \\
\bottomrule
\end{tabular}
\note{CORRECTED: Tasks have different maximum scores. Bold entries show corrections from previous version.}
\end{table}

% CORRECTED Main Results Table - ALL EVALUATION MODES INCLUDED
\begin{table*}[htbp]
\centering
\caption{CORRECTED Comprehensive Performance Results - ALL Evaluation Modes}
\label{tab:comprehensive_results_corrected}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llcccccc@{}}
\toprule
\textbf{Model} & \textbf{Evaluation Mode} & \textbf{Accuracy (\%)} & \textbf{Quality Score (\%)} & \textbf{Avg Score Distance} & \textbf{Total Cost (\$)} & \textbf{Evaluations} & \textbf{Avg Time (s)} \\
\midrule
\multirow{3}{*}{OpenAI o4-mini} 
& Without Answer & 55.74 & 75.55 & 0.66 & 2.1788 & 122 & 39.62 \\
& With Answer & \textbf{56.56} & \textbf{78.17} & 0.60 & 2.0174 & 122 & 32.94 \\
& With True Solution & 54.10 & 76.16 & 0.66 & 2.2779 & 122 & 58.47 \\
\midrule
\multirow{3}{*}{Google Gemini 2.0 Flash} 
& Without Answer & 36.89 & 71.04 & 0.84 & 0.1422 & 122 & 4.56 \\
& With Answer & 47.54 & 74.04 & 0.75 & 0.1445 & 122 & 4.82 \\
& With True Solution & 46.72 & 75.82 & 0.71 & 0.2057 & 122 & 3.13 \\
\midrule
\multirow{3}{*}{Google Gemini 2.0 Flash Lite} 
& Without Answer & 31.15 & 67.21 & 0.95 & 0.0184 & 122 & 3.25 \\
& With Answer & 40.98 & 70.49 & 0.84 & 0.0185 & 122 & 3.09 \\
& With True Solution & 38.52 & 70.22 & 0.84 & \textbf{0.0369} & 122 & \textbf{3.09} \\
\midrule
\multirow{3}{*}{Google Gemini 2.5 Flash Preview} 
& Without Answer & 31.97 & 67.21 & 0.95 & 0.1722 & 122 & 11.67 \\
& With Answer & 40.98 & 70.49 & 0.84 & 0.1722 & 122 & 11.67 \\
& With True Solution & 45.90 & 71.35 & 0.79 & 0.3444 & 122 & 11.67 \\
\midrule
\multirow{3}{*}{Google Gemini 2.5 Flash Preview:thinking} 
& Without Answer & 31.97 & 65.30 & 1.01 & 0.3917 & 122 & 47.59 \\
& With Answer & 40.98 & 68.85 & 0.89 & 0.3917 & 122 & 47.59 \\
& With True Solution & 43.44 & 65.92 & 0.99 & 0.7833 & 122 & 47.59 \\
\midrule
\multirow{3}{*}{Qwen 2.5 VL 32B} 
& Without Answer & 31.15 & 62.09 & 1.09 & 0.4550 & 122 & 22.97 \\
& With Answer & 30.33 & 61.95 & 1.08 & 0.4571 & 122 & 23.27 \\
& With True Solution & 43.44 & 70.49 & 0.81 & 0.6344 & 122 & 27.55 \\
\midrule
\multirow{3}{*}{Arcee AI Spotlight} 
& Without Answer & 27.87 & 64.48 & 1.04 & 0.0000 & 122 & 8.80 \\
& With Answer & 26.23 & 63.18 & 1.09 & 0.0000 & 122 & 6.99 \\
& With True Solution & 25.41 & 59.22 & 1.16 & 0.0000 & 122 & 6.98 \\
\bottomrule
\end{tabular}%
}
\note{CORRECTED: ALL evaluation modes included for ALL models. Quality Score uses task-specific normalization.}
\end{table*}

% CORRECTED Model Rankings Table
\begin{table}[htbp]
\centering
\caption{CORRECTED Model Performance Rankings by Average Quality Score}
\label{tab:model_rankings_corrected}
\begin{tabular}{@{}clccc@{}}
\toprule
\textbf{Rank} & \textbf{Model} & \textbf{Avg Quality Score (\%)} & \textbf{Total Cost (\$)} & \textbf{Total Evaluations} \\
\midrule
1 & OpenAI o4-mini & 76.63 & 6.4741 & 366 \\
2 & Google Gemini 2.0 Flash & 73.63 & 0.4924 & 366 \\
3 & Google Gemini 2.5 Flash Preview & 69.68 & 0.6888 & 366 \\
4 & Google Gemini 2.0 Flash Lite & 69.31 & 0.0738 & 366 \\
5 & Google Gemini 2.5 Flash Preview:thinking & 66.69 & 1.5667 & 366 \\
6 & Qwen 2.5 VL 32B & 64.84 & 1.5465 & 366 \\
7 & Arcee AI Spotlight & 62.29 & 0.0000 & 366 \\
\bottomrule
\end{tabular}
\note{CORRECTED: Rankings based on average quality score across ALL available evaluation modes.}
\end{table}

% CORRECTED Cost-Effectiveness Analysis Table
\begin{table}[htbp]
\centering
\caption{CORRECTED Cost-Effectiveness Analysis of AI Models}
\label{tab:cost_effectiveness_corrected}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model} & \textbf{Cost per Evaluation (\$)} & \textbf{Avg Quality Score (\%)} & \textbf{Quality/Cost Ratio} \\
\midrule
Arcee AI Spotlight & 0.0000 & 62.29 & $\infty$ \\
Google Gemini 2.0 Flash Lite & 0.0002 & 69.31 & 346,550 \\
Google Gemini 2.0 Flash & 0.0013 & 73.63 & 56,638 \\
Google Gemini 2.5 Flash Preview & 0.0019 & 69.68 & 36,674 \\
Qwen 2.5 VL 32B & 0.0042 & 64.84 & 15,438 \\
Google Gemini 2.5 Flash Preview:thinking & 0.0043 & 66.69 & 15,509 \\
OpenAI o4-mini & 0.0177 & 76.63 & 4,330 \\
\bottomrule
\end{tabular}
\note{CORRECTED: Quality/Cost ratio calculated using average quality scores across all modes.}
\end{table}

% CORRECTED Evaluation Modes Comparison Table
\begin{table}[htbp]
\centering
\caption{CORRECTED Performance Comparison Across Evaluation Modes}
\label{tab:evaluation_modes_corrected}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Model} & \textbf{Without Answer} & \textbf{With Answer} & \textbf{With True Solution} \\
\midrule
\multicolumn{4}{c}{\textit{Quality Score (\%) - CORRECTED}} \\
\midrule
OpenAI o4-mini & 75.55 & \textbf{78.17} & 76.16 \\
Google Gemini 2.0 Flash & 71.04 & 74.04 & \textbf{75.82} \\
Google Gemini 2.0 Flash Lite & 67.21 & \textbf{70.49} & 70.22 \\
Google Gemini 2.5 Flash Preview & 67.21 & 70.49 & \textbf{71.35} \\
Google Gemini 2.5 Flash Preview:thinking & 65.30 & 68.85 & \textbf{65.92} \\
Qwen 2.5 VL 32B & 62.09 & 61.95 & \textbf{70.49} \\
Arcee AI Spotlight & \textbf{64.48} & 63.18 & 59.22 \\
\midrule
\multicolumn{4}{c}{\textit{Accuracy (\%) - CORRECTED}} \\
\midrule
OpenAI o4-mini & 55.74 & \textbf{56.56} & 54.10 \\
Google Gemini 2.0 Flash & 36.89 & \textbf{47.54} & 46.72 \\
Google Gemini 2.0 Flash Lite & 31.15 & \textbf{40.98} & 38.52 \\
Google Gemini 2.5 Flash Preview & 31.97 & \textbf{40.98} & 45.90 \\
Google Gemini 2.5 Flash Preview:thinking & 31.97 & \textbf{40.98} & 43.44 \\
Qwen 2.5 VL 32B & 31.15 & 30.33 & \textbf{43.44} \\
Arcee AI Spotlight & \textbf{27.87} & 26.23 & 25.41 \\
\bottomrule
\end{tabular}
\note{CORRECTED: ALL models now show ALL available evaluation modes. Best performance for each model highlighted in bold.}
\end{table}

% CORRECTED Metrics Explanation Table
\begin{table}[htbp]
\centering
\caption{CORRECTED Explanation of Quality Score Metric}
\label{tab:quality_score_explanation}
\begin{tabular}{p{12cm}}
\toprule
\textbf{Quality Score: CORRECTED Calculation Method} \\
\midrule
The Quality Score is a normalized measure that indicates how close the model's predictions are to the expected scores, taking into account the task-specific maximum possible error. \\
\addlinespace
\textbf{CORRECTED Calculation:} \\
1. For each prediction, calculate the absolute difference between predicted and expected score \\
2. Normalize this difference by dividing by the maximum possible score for the specific task type: \\
   - Tasks 13, 15: max score = 2 points \\
   - Tasks 14, 16, 17: max score = 3 points \\
   - Tasks 18, 19: max score = 4 points \\
3. Subtract this normalized distance from 1 to get a quality score between 0 and 1 \\
4. Convert to percentage by multiplying by 100 \\
\addlinespace
\textbf{Formula:} Quality Score = 100\% Ã— (1 - |predicted - expected| / max\_score\_for\_task) \\
\addlinespace
This ensures fair comparison across tasks with different scoring scales. \\
\bottomrule
\end{tabular}
\end{table}

\end{document}
