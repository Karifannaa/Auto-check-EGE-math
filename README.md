# Auto-check-EGE-math

Система автоматической проверки решений задач ЕГЭ по математике с использованием моделей искусственного интеллекта.

## Описание проекта

**Auto-check-EGE-math** — это комплексная система для автоматической оценки решений задач Единого государственного экзамена (ЕГЭ) по математике. Система использует современные модели машинного обучения через OpenRouter API для анализа изображений с решениями студентов и выставления оценок в соответствии с критериями ЕГЭ.

### Основные возможности

- **Автоматическая оценка решений**: Анализ изображений с решениями задач ЕГЭ по математике
- **Поддержка различных типов задач**: Задачи 13-19 (тригонометрия, стереометрия, логарифмы, планиметрия, финансовая математика, параметры, теория чисел)
- **Множественные модели ИИ**: Поддержка reasoning и non-reasoning моделей через OpenRouter API
- **Бенчмаркинг**: Система для тестирования производительности различных моделей
- **Веб-интерфейс**: FastAPI backend с возможностью интеграции фронтенда
- **Анализ стоимости**: Расчет затрат на использование различных моделей

### Поддерживаемые типы задач

| Номер задачи | Описание | Максимальный балл |
|--------------|----------|-------------------|
| 13 | Тригонометрические уравнения | 2 |
| 14 | Стереометрическая задача | 3 |
| 15 | Логарифмические неравенства | 2 |
| 16 | Финансовая математика | 2 |
| 17 | Планиметрическая задача | 3 |
| 18 | Задача с параметром | 4 |
| 19 | Задача по теории чисел | 4 |

## Архитектура системы

Проект состоит из следующих основных компонентов:

### Backend (FastAPI)
- **API для оценки решений**: Основной endpoint для загрузки и оценки решений
- **Управление моделями**: Получение списка доступных моделей и их характеристик
- **Интеграция с OpenRouter**: Клиент для работы с различными моделями ИИ
- **Обработка изображений**: Утилиты для подготовки изображений к анализу
- **Генерация промптов**: Специализированные промпты для разных типов задач

### Frontend (React + TypeScript)
- **Веб-интерфейс**: Интерфейс для загрузки решений и просмотра результатов
- **Современный стек**: React 19, TypeScript, Vite
- **Responsive дизайн**: Адаптивный интерфейс для различных устройств

### Система бенчмаркинга
- **Тестирование моделей**: Автоматическое тестирование различных моделей ИИ
- **Анализ результатов**: Подробная статистика производительности
- **Датасет**: Коллекция решений с эталонными оценками
- **Метрики качества**: Точность, precision, recall, F1-score

## Информация о датасете

### Обзор датасета

**Название:** Russian Math Exam Solutions Benchmark Dataset
**Источник:** Собственная коллекция решений задач ЕГЭ по математике
**Версия:** 1.0
**Формат:** HuggingFace Dataset
**Лицензия:** Только для исследовательских целей

Датасет содержит **122 примера** студенческих решений задач Единого государственного экзамена (ЕГЭ) по математике с эталонными оценками для тестирования систем автоматической проверки.

### Структура датасета

#### Распределение по типам задач

| Тип задачи | Описание | Количество примеров | Максимальный балл |
|------------|----------|-------------------|-------------------|
| 13 | Тригонометрические уравнения | 21 | 2 |
| 14 | Стереометрическая задача | 18 | 3 |
| 15 | Логарифмические неравенства | 19 | 2 |
| 16 | Финансовая математика | 17 | 2 |
| 17 | Планиметрическая задача | 15 | 3 |
| 18 | Задача с параметром | 16 | 4 |
| 19 | Задача по теории чисел | 16 | 4 |

#### Распределение оценок

| Балл | Количество | Процент |
|------|------------|---------|
| 0 | 28 | 22.95% |
| 1 | 40 | 32.79% |
| 2 | 35 | 28.69% |
| 3 | 11 | 9.02% |
| 4 | 8 | 6.56% |

### Формат данных

Каждый пример в датасете содержит следующие поля:

- **`solution_id`** (string): Уникальный идентификатор решения (например, "13.1.1")
- **`task_id`** (string): Номер типа задачи ("13"-"19")
- **`example_id`** (string): Идентификатор конкретного примера задачи (например, "13.1")
- **`task_type`** (string): Описание типа задачи на русском языке
- **`score`** (int64): Эталонная оценка (0-4 балла)
- **`parts_count`** (int64): Количество частей в решении (1-2)
- **`images_with_answer`** (Sequence[string]): Пути к изображениям с решением и правильным ответом
- **`images_without_answer`** (Sequence[string]): Пути к изображениям только с решением студента
- **`images_with_true_solution`** (Sequence[string]): Пути к изображениям с эталонным решением (если доступно)

### Спецификации изображений

- **Формат:** PNG
- **Разрешение:** Переменное (оптимизировано для читаемости)
- **Цветовая схема:** RGB
- **Размер файла:** 50KB - 2MB в зависимости от сложности решения
- **Качество:** Высокое разрешение для четкого распознавания математических формул и текста

### Методология сбора данных

1. **Источник:** Реальные решения студентов на экзаменах ЕГЭ по математике
2. **Оценка:** Проведена квалифицированными экспертами согласно официальным критериям ФИПИ
3. **Обработка:**
   - Сканирование решений в высоком разрешении
   - Создание версий с ответами и без ответов
   - Добавление эталонных решений для сравнения
   - Валидация оценок несколькими экспертами

### Предобработка данных

1. **Обработка изображений:**
   - Улучшение контрастности (коэффициент 1.3)
   - Изменение размера до максимум 2048px для оптимизации
   - Конвертация в base64 для API-запросов

2. **Структурирование данных:**
   - Создание метаданных в JSON формате
   - Организация файлов по типам задач и примерам
   - Генерация HuggingFace Dataset для удобного доступа

### Доступ к датасету

#### Локальный доступ

<augment_code_snippet path="README.md" mode="EXCERPT">
````python
from datasets import load_from_disk

# Загрузка основного датасета
dataset = load_from_disk("dataset_benchmark_hf")

# Загрузка обновленного датасета с эталонными решениями
dataset_updated = load_from_disk("dataset_benchmark_hf_updated")

# Просмотр примера
example = dataset[0]
print(f"Solution ID: {example['solution_id']}")
print(f"Task: {example['task_type']}")
print(f"Score: {example['score']}")
print(f"Images: {len(example['images_without_answer'])}")
````
</augment_code_snippet>

#### Анализ датасета

<augment_code_snippet path="README.md" mode="EXCERPT">
````bash
# Анализ структуры датасета
python analyze_dataset.py

# Анализ конкретной версии
python analyze_dataset.py dataset_benchmark_hf_updated
````
</augment_code_snippet>

### Структура файлов датасета

```
dataset_benchmark/
├── metadata.json                 # Полные метаданные всех примеров
├── metadata.csv                  # Метаданные в CSV формате
├── 13/                          # Задачи типа 13
│   ├── 13.1/                    # Пример 13.1
│   │   ├── 13.1_solve_true.png  # Эталонное решение
│   │   ├── 13.1.1/              # Решение студента 1
│   │   │   ├── with_answer/     # С правильным ответом
│   │   │   └── without_answer/  # Только решение
│   │   └── 13.1.2/              # Решение студента 2
│   └── 13.2/                    # Пример 13.2
├── 14/                          # Задачи типа 14
└── ...                          # Остальные типы задач

dataset_benchmark_hf/            # HuggingFace Dataset
├── README.md                    # Описание датасета
├── dataset_info.json           # Информация о структуре
├── data-00000-of-00001.arrow    # Данные в формате Arrow
├── state.json                   # Состояние датасета
├── with_answer/                 # Изображения с ответами
├── without_answer/              # Изображения без ответов
└── with_true_solution/          # Эталонные решения
```

## Технологический стек

### Backend
- **Python 3.8+**
- **FastAPI** - современный веб-фреймворк для API
- **OpenRouter API** - доступ к различным моделям ИИ
- **Pillow** - обработка изображений
- **Pydantic** - валидация данных
- **httpx** - асинхронные HTTP-запросы

### Frontend
- **React 19** - библиотека для создания пользовательских интерфейсов
- **TypeScript** - типизированный JavaScript
- **Vite** - быстрый инструмент сборки
- **Axios** - HTTP-клиент для API запросов

### Бенчмаркинг и данные
- **HuggingFace Datasets** - работа с датасетами
- **NumPy** - численные вычисления
- **Pandas** - анализ данных
- **JSON** - хранение метаданных и результатов

## Быстрый старт

### Предварительные требования

1. **Python 3.8+**
2. **Node.js 16+** (для фронтенда)
3. **OpenRouter API ключ** - получите на [openrouter.ai](https://openrouter.ai)

### Установка и запуск

#### 1. Клонирование репозитория
```bash
git clone https://github.com/Karifannaa/Auto-check-EGE-math.git
cd Auto-check-EGE-math
```

#### 2. Настройка backend
```bash
cd backend

# Создание виртуального окружения
python -m venv venv
source venv/bin/activate  # На Windows: venv\Scripts\activate

# Установка зависимостей
pip install -r requirements.txt

# Настройка переменных окружения
cp .env.example .env
# Отредактируйте .env файл, добавив ваш OpenRouter API ключ
```

#### 3. Запуск backend
```bash
uvicorn app.main:app --reload
```

Backend будет доступен по адресу: http://localhost:8000

#### 4. Настройка frontend (опционально)
```bash
cd ../frontend

# Установка зависимостей
npm install

# Запуск в режиме разработки
npm run dev
```

Frontend будет доступен по адресу: http://localhost:5173

## API Endpoints

### Основные endpoints

#### Оценка решений
- `POST /api/v1/solutions/evaluate` - Оценить решение задачи

**Параметры:**
- `task_type` - тип задачи (task_13, task_14, и т.д.)
- `model_id` - идентификатор модели
- `student_solution_image` - изображение с решением студента
- `correct_solution_image` - изображение с правильным решением (опционально)
- `temperature` - температура модели (по умолчанию 0.7)
- `max_tokens` - максимальное количество токенов (по умолчанию 10000)

#### Управление моделями
- `GET /api/v1/models/available` - Получить список доступных моделей
- `GET /api/v1/models/task-types` - Получить список типов задач
- `GET /api/v1/models/credits` - Получить информацию о кредитах аккаунта
- `GET /api/v1/models/cost-estimate/{model_id}` - Оценка стоимости использования модели

### Веб-интерфейс
- `GET /` - Главная страница с интерфейсом для оценки решений
- `GET /batch` - Интерфейс для пакетной обработки

## Структура проекта

```
Auto-check-EGE-math/
├── backend/                          # Backend приложение
│   ├── app/
│   │   ├── api/                      # API клиенты
│   │   │   └── openrouter_client.py  # Клиент OpenRouter
│   │   ├── core/                     # Основная конфигурация
│   │   │   └── config.py             # Настройки приложения
│   │   ├── models/                   # Модели данных
│   │   │   └── solution.py           # Модели для решений
│   │   ├── routers/                  # API маршруты
│   │   │   ├── solutions.py          # Маршруты для решений
│   │   │   ├── models.py             # Маршруты для моделей
│   │   │   └── web.py                # Веб-интерфейс
│   │   ├── utils/                    # Утилиты
│   │   │   ├── image_utils.py        # Обработка изображений
│   │   │   ├── prompt_utils.py       # Генерация промптов
│   │   │   ├── cost_calculator.py    # Расчет стоимости
│   │   │   └── score_extractor.py    # Извлечение оценок
│   │   ├── static/                   # Статические файлы
│   │   ├── templates/                # HTML шаблоны
│   │   └── main.py                   # Точка входа
│   ├── tests/                        # Тесты
│   ├── requirements.txt              # Python зависимости
│   └── README.md                     # Документация backend
├── frontend/                         # Frontend приложение
│   ├── src/                          # Исходный код
│   ├── public/                       # Публичные файлы
│   ├── package.json                  # Node.js зависимости
│   └── README.md                     # Документация frontend
├── dataset_benchmark/                # Система бенчмаркинга
│   ├── benchmark_models.py           # Основной модуль бенчмаркинга
│   ├── run_full_benchmark.py         # Запуск полного бенчмарка
│   ├── run_task13_benchmark.py       # Бенчмарк для задачи 13
│   ├── analyze_existing_results.py   # Анализ результатов
│   ├── metadata.json                 # Метаданные датасета
│   ├── benchmark_results/            # Результаты бенчмарков
│   └── [13-19]/                      # Папки с задачами по типам
├── dataset_benchmark_hf/             # HuggingFace датасет
├── fipi_examples_of_estimation/      # Примеры оценивания ФИПИ
└── README.md                         # Основная документация
```

## Бенчмаркинг моделей

Система включает в себя мощный инструмент для тестирования производительности различных моделей ИИ на датасете решений ЕГЭ.

### Предварительные требования для бенчмаркинга

1. **Настроенный backend** с OpenRouter API ключом
2. **Датасет** - автоматически загружается при первом запуске
3. **Достаточный баланс** на OpenRouter аккаунте для тестирования моделей

### Доступные модели для тестирования

#### Reasoning модели (с поддержкой мышления)
- `moonshotai/kimi-vl-a3b-thinking:free` - Бесплатная модель Kimi с визуальными возможностями
- `google/gemini-2.5-flash-preview:thinking` - Gemini с режимом мышления
- `google/gemini-2.0-flash-exp` - Экспериментальная версия Gemini

#### Non-reasoning модели
- `qwen/qwen2.5-vl-32b-instruct:free` - Бесплатная модель Qwen с визуальными возможностями
- `gpt-4o` - GPT-4 Omni от OpenAI
- `gpt-4o-mini` - Облегченная версия GPT-4 Omni

### Запуск бенчмарков

#### 1. Полный бенчмарк всех задач

```bash
cd dataset_benchmark

# Запуск с бесплатными моделями (рекомендуется для начала)
python run_full_benchmark.py

# Запуск с конкретными моделями
python run_full_benchmark.py --models "moonshotai/kimi-vl-a3b-thinking:free" "google/gemini-2.0-flash-exp"

# Ограничение количества примеров для тестирования
python run_full_benchmark.py --max-examples 5
```

#### 2. Бенчмарк конкретной задачи (например, задача 13)

```bash
# Тестирование только задачи 13
python run_task13_benchmark.py

# С конкретными параметрами
python run_task13_benchmark.py --models "google/gemini-2.0-flash-exp" --max-examples 10
```

#### 3. Бенчмарк с использованием эталонных решений

```bash
# Запуск бенчмарка с эталонными решениями для сравнения
python benchmark_models.py --task-id 13 --models "google/gemini-2.0-flash-exp" --use-true-solution
```

### Параметры бенчмаркинга

#### Основные параметры

- `--models` - Список моделей для тестирования
- `--max-examples` - Максимальное количество примеров на задачу
- `--task-id` - Конкретная задача для тестирования (13-19)
- `--use-true-solution` - Использовать эталонные решения для сравнения
- `--prompt-variant` - Вариант промпта (basic, detailed, with_solution)
- `--temperature` - Температура модели (по умолчанию 0.7)

#### Примеры команд

```bash
# Быстрое тестирование с ограниченным количеством примеров
python run_full_benchmark.py --max-examples 3

# Тестирование конкретной модели на всех задачах
python run_full_benchmark.py --models "moonshotai/kimi-vl-a3b-thinking:free"

# Детальное тестирование задачи 13 с эталонными решениями
python benchmark_models.py --task-id 13 --use-true-solution --prompt-variant detailed

# Тестирование нескольких моделей с разными температурами
python benchmark_models.py --models "google/gemini-2.0-flash-exp" "gpt-4o-mini" --temperature 0.3
```

### Интерпретация результатов

#### Файлы результатов

После завершения бенчмарка создаются следующие файлы:

1. **Основные результаты**: `benchmark_results/benchmark_[model]_[timestamp].json`
2. **Анализ**: `benchmark_results/benchmark_[model]_[timestamp]_analysis.json`
3. **Таблицы метрик**: `benchmark_results/benchmark_[model]_[timestamp]_metrics_table.tex`

#### Ключевые метрики

- **Accuracy (Точность)**: Процент правильно оцененных решений
- **Quality Score**: Качественная оценка на основе близости к эталонной оценке
- **Precision/Recall/F1**: Метрики для каждого балла отдельно
- **Average Score Distance**: Среднее отклонение от правильной оценки
- **Confusion Matrix**: Матрица ошибок для детального анализа

#### Пример вывода результатов

```
Benchmark Summary:
Total examples: 122
Total evaluations: 244
Total cost: $2.4567
Average quality score: 78.45%

Model Performance:

google/gemini-2.0-flash-exp:
  with_answer:
    Accuracy: 73.77%
    Quality score: 81.25%
    Avg. score distance: 0.43
    Macro precision: 75.20%
    Macro recall: 71.80%
    Macro F1: 73.45%
    Evaluations: 122
    Avg. evaluation time: 8.45s
    Total cost: $1.2345
```

### Анализ существующих результатов

```bash
# Анализ уже сохраненных результатов
python analyze_existing_results.py

# Сравнение результатов разных моделей
python analyze_existing_results.py --compare-models
```

### Устранение неполадок

#### Частые проблемы и решения

1. **Ошибка "Rate limit exceeded"**
   ```
   Решение: Система автоматически повторяет запросы с экспоненциальной задержкой.
   Если проблема сохраняется, уменьшите количество одновременных запросов или используйте модели с более высокими лимитами.
   ```

2. **Ошибка "OpenRouter API key not configured"**
   ```bash
   # Убедитесь, что API ключ настроен в backend/.env
   echo "OPENROUTER_API_KEY=your_key_here" >> backend/.env
   ```

3. **Ошибка "Dataset not found"**
   ```bash
   # Убедитесь, что датасет находится в правильной папке
   ls dataset_benchmark_hf/
   # Если датасет отсутствует, он будет загружен автоматически при первом запуске
   ```

4. **Недостаточно средств на аккаунте**
   ```
   Решение: Пополните баланс на OpenRouter или используйте бесплатные модели:
   - moonshotai/kimi-vl-a3b-thinking:free
   - qwen/qwen2.5-vl-32b-instruct:free
   ```

5. **Медленная работа бенчмарка**
   ```bash
   # Используйте ограничение количества примеров для быстрого тестирования
   python run_full_benchmark.py --max-examples 5

   # Или тестируйте только одну задачу
   python run_task13_benchmark.py --max-examples 10
   ```

### Рекомендации по использованию

1. **Начните с бесплатных моделей** для ознакомления с системой
2. **Используйте ограничение примеров** (`--max-examples`) для быстрого тестирования
3. **Сохраняйте результаты** для последующего анализа и сравнения
4. **Мониторьте расходы** через OpenRouter dashboard
5. **Анализируйте confusion matrix** для понимания типов ошибок модели

## Методология бенчмаркинга

### Обзор методологии

Система бенчмаркинга предназначена для комплексной оценки производительности моделей ИИ на задачах автоматической проверки решений ЕГЭ по математике. Методология основана на сравнении оценок модели с эталонными оценками экспертов.

### Архитектура бенчмарка

#### Основные компоненты

1. **ModelBenchmark** - основной класс для проведения бенчмарков
2. **OpenRouterClient** - клиент для взаимодействия с моделями через OpenRouter API
3. **PromptGenerator** - генератор специализированных промптов для разных типов задач
4. **Система метрик** - расчет точности, precision, recall, F1-score и других показателей

#### Типы оценки

1. **С ответом** (`with_answer`): Модель видит решение студента с правильным ответом
2. **Без ответа** (`without_answer`): Модель видит только решение студента
3. **С эталонным решением** (`with_true_solution`): Модель видит эталонное решение для сравнения

### Метрики оценки

#### Основные метрики

1. **Accuracy (Точность)**: Процент правильно оцененных решений
   ```
   Accuracy = (Правильные оценки) / (Общее количество оценок) × 100%
   ```

2. **Quality Score**: Качественная оценка на основе близости к эталонной оценке
   ```
   Quality Score = 1 - (|Оценка модели - Эталонная оценка|) / Максимальный балл
   ```

3. **Average Score Distance**: Среднее отклонение от правильной оценки
   ```
   Avg Distance = Σ|Оценка модели - Эталонная оценка| / Количество оценок
   ```

#### Детальные метрики

Для каждого возможного балла (0-4) рассчитываются:

- **Precision**: TP / (TP + FP) - точность предсказания конкретного балла
- **Recall**: TP / (TP + FN) - полнота предсказания конкретного балла
- **F1-Score**: 2 × (Precision × Recall) / (Precision + Recall)

#### Макро-метрики

- **Macro Precision**: Среднее арифметическое precision по всем баллам
- **Macro Recall**: Среднее арифметическое recall по всем баллам
- **Macro F1**: Среднее арифметическое F1-score по всем баллам

### Процедура выполнения бенчмарка

#### 1. Инициализация

<augment_code_snippet path="README.md" mode="EXCERPT">
````python
# Создание экземпляра бенчмарка
benchmark = ModelBenchmark(
    dataset_dir="dataset_benchmark_hf",
    results_dir="benchmark_results",
    max_retries=10,
    initial_delay=15,
    max_delay=120
)
````
</augment_code_snippet>

#### 2. Фильтрация данных

<augment_code_snippet path="README.md" mode="EXCERPT">
````python
# Фильтрация по типу задачи
filtered_data = benchmark.filter_dataset(task_id="13")

# Ограничение количества примеров
if max_examples:
    filtered_data = filtered_data[:max_examples]
````
</augment_code_snippet>

#### 3. Обработка изображений

Для каждого примера выполняется:

1. **Загрузка изображений** из файловой системы
2. **Предобработка**:
   - Изменение размера до максимум 2048px
   - Улучшение контрастности (коэффициент 1.3)
   - Конвертация в base64 для API
3. **Создание составных сообщений** для многочастных решений

#### 4. Генерация промптов

Система использует специализированные промпты для каждого типа задачи:

- **basic**: Базовый промпт для оценки
- **detailed**: Детальный промпт с объяснением критериев
- **with_solution**: Промпт для сравнения с эталонным решением

#### 5. Вызов API модели

<augment_code_snippet path="README.md" mode="EXCERPT">
````python
# Вызов API с обработкой ошибок и повторными попытками
response = await client.chat_completion(
    model=model_id,
    messages=messages,
    temperature=temperature,
    max_tokens=max_tokens,
    extra_body=extra_body  # Для thinking моделей
)
````
</augment_code_snippet>

#### 6. Извлечение оценки

Система использует регулярные выражения для извлечения оценки из ответа модели:

<augment_code_snippet path="README.md" mode="EXCERPT">
````python
# Паттерны для поиска оценки
score_patterns = [
    r'итоговая оценка[\s\S]*?(\d+)\s*балл',
    r'оценка:\s*(\d+)',
    r'(\d+)\s*балл',
    r'\[оценка\s*[:-]\s*(\d+)\]'
]
````
</augment_code_snippet>

### Обработка ошибок и ограничений

#### Rate Limiting

Система автоматически обрабатывает ограничения скорости API:

1. **Экспоненциальная задержка**: Увеличение времени ожидания при повторных попытках
2. **Jitter**: Случайное отклонение времени ожидания (±20%)
3. **Максимальные попытки**: Настраиваемое количество повторных попыток
4. **Логирование**: Детальное логирование всех попыток и ошибок

#### Валидация оценок

- **Проверка диапазона**: Оценки ограничиваются максимальными баллами для каждого типа задачи
- **Обработка некорректных ответов**: Присвоение 0 баллов при невозможности извлечь оценку
- **Логирование предупреждений**: Запись всех случаев некорректных оценок

### Формат результатов

#### Основной файл результатов

<augment_code_snippet path="README.md" mode="EXCERPT">
````json
{
  "metadata": {
    "timestamp": "2025-01-11T15:30:45",
    "model_id": "google/gemini-2.0-flash-exp",
    "task_id": "13",
    "total_examples": 21,
    "parameters": {
      "temperature": 0.7,
      "max_tokens": 10000,
      "use_true_solution": true
    }
  },
  "results": [
    {
      "solution_id": "13.1.1",
      "expected_score": 2,
      "predicted_score": 2,
      "evaluation_time": 12.5,
      "cost": 0.0045,
      "tokens": {
        "prompt": 2594,
        "completion": 365,
        "total": 2959
      }
    }
  ]
}
````
</augment_code_snippet>

#### Файл анализа

<augment_code_snippet path="README.md" mode="EXCERPT">
````json
{
  "total_examples": 21,
  "models": {
    "google/gemini-2.0-flash-exp": {
      "with_true_solution": {
        "accuracy": 47.62,
        "quality_score": 66.67,
        "avg_score_distance": 0.67,
        "macro_precision": 57.94,
        "macro_recall": 38.89,
        "macro_f1": 37.81,
        "confusion_matrix": {
          "0": {"0": 1, "1": 2, "2": 3},
          "1": {"0": 0, "1": 8, "2": 2},
          "2": {"0": 0, "1": 4, "2": 1}
        }
      }
    }
  }
}
````
</augment_code_snippet>

### Процедуры валидации

#### Валидация датасета

1. **Проверка целостности данных**:
   <augment_code_snippet path="README.md" mode="EXCERPT">
   ````bash
   # Проверка структуры датасета
   python dataset_benchmark/verify_dataset.py

   # Анализ метаданных
   python dataset_benchmark/generate_metadata.py --verify
   ````
   </augment_code_snippet>

2. **Валидация изображений**:
   - Проверка существования всех файлов изображений
   - Валидация формата и читаемости изображений
   - Проверка соответствия путей в метаданных

3. **Валидация оценок**:
   - Проверка диапазона оценок для каждого типа задачи
   - Валидация соответствия оценок критериям ФИПИ

#### Валидация результатов бенчмарка

1. **Статистическая валидация**:
   - Проверка распределения оценок
   - Анализ выбросов и аномальных результатов
   - Валидация метрик (сумма precision/recall/F1)

2. **Воспроизводимость**:
   - Повторные запуски с одинаковыми параметрами
   - Проверка стабильности результатов
   - Документирование всех параметров запуска

### Ограничения и соображения

#### Ограничения датасета

1. **Размер выборки**: 122 примера могут быть недостаточны для некоторых типов анализа
2. **Дисбаланс классов**: Неравномерное распределение оценок (больше низких оценок)
3. **Субъективность оценки**: Человеческие оценки могут содержать субъективные элементы
4. **Ограниченные типы задач**: Только задачи 13-19 из ЕГЭ по математике

#### Ограничения бенчмарка

1. **Зависимость от API**: Результаты зависят от доступности и стабильности OpenRouter API
2. **Стоимость тестирования**: Некоторые модели требуют значительных затрат
3. **Rate Limiting**: Ограничения скорости API могут замедлить выполнение
4. **Изменчивость моделей**: Модели могут обновляться, влияя на воспроизводимость

#### Интерпретация результатов

1. **Контекстуальность**: Результаты специфичны для русскоязычных математических задач ЕГЭ
2. **Сравнительность**: Результаты лучше использовать для сравнения моделей, а не абсолютной оценки
3. **Временная зависимость**: Результаты могут изменяться при обновлении моделей

### Практические рекомендации

#### Для исследователей

1. **Начальное тестирование**:
   <augment_code_snippet path="README.md" mode="EXCERPT">
   ````bash
   # Быстрый тест с бесплатными моделями
   python run_task13_benchmark.py --models "moonshotai/kimi-vl-a3b-thinking:free" --max-examples 5
   ````
   </augment_code_snippet>

2. **Полное исследование**:
   <augment_code_snippet path="README.md" mode="EXCERPT">
   ````bash
   # Полный бенчмарк всех задач
   python run_full_benchmark.py --models "google/gemini-2.0-flash-exp" "gpt-4o"
   ````
   </augment_code_snippet>

3. **Анализ результатов**:
   <augment_code_snippet path="README.md" mode="EXCERPT">
   ````bash
   # Сравнительный анализ
   python analyze_existing_results.py --compare-models
   ````
   </augment_code_snippet>

#### Для разработчиков

1. **Интеграция с собственными моделями**:
   - Адаптация `OpenRouterClient` для работы с локальными моделями
   - Модификация `PromptGenerator` для специфических промптов
   - Расширение системы метрик

2. **Расширение датасета**:
   - Добавление новых примеров через `generate_metadata.py`
   - Обновление HuggingFace Dataset через `prepare_huggingface_dataset.py`
   - Валидация новых данных

#### Для преподавателей

1. **Использование в образовательных целях**:
   - Демонстрация автоматической проверки решений
   - Анализ типичных ошибок студентов
   - Сравнение подходов к решению задач

2. **Создание учебных материалов**:
   - Использование примеров из датасета
   - Анализ критериев оценивания
   - Демонстрация возможностей ИИ в образовании

### Команды для быстрого старта

#### Анализ датасета
<augment_code_snippet path="README.md" mode="EXCERPT">
````bash
# Общая статистика
python analyze_dataset.py

# Проверка целостности
python dataset_benchmark/verify_dataset.py

# Генерация отчета
python dataset_benchmark/debug_paths.py
````
</augment_code_snippet>

#### Запуск бенчмарков
<augment_code_snippet path="README.md" mode="EXCERPT">
````bash
# Быстрый тест (5 минут)
python run_task13_benchmark.py --models "moonshotai/kimi-vl-a3b-thinking:free" --max-examples 3

# Средний тест (30 минут)
python run_full_benchmark.py --models "qwen/qwen2.5-vl-32b-instruct:free" --max-examples 5

# Полный тест (несколько часов)
python run_full_benchmark.py --models "google/gemini-2.0-flash-exp" "gpt-4o"
````
</augment_code_snippet>

#### Анализ результатов
<augment_code_snippet path="README.md" mode="EXCERPT">
````bash
# Просмотр последних результатов
ls -la dataset_benchmark/benchmark_results/

# Анализ конкретного результата
python analyze_existing_results.py --file benchmark_results/benchmark_task13_*.json

# Сравнение моделей
python analyze_existing_results.py --compare-models
````
</augment_code_snippet>

### Файлы конфигурации и метаданных

#### Основные файлы

- **`dataset_benchmark/metadata.json`**: Полные метаданные всех примеров
- **`dataset_benchmark/metadata_schema.json`**: Схема структуры метаданных
- **`dataset_benchmark_hf/dataset_info.json`**: Информация о HuggingFace Dataset
- **`backend/openrouter_models.json`**: Список доступных моделей OpenRouter

#### Файлы результатов

- **`benchmark_results/benchmark_*.json`**: Основные результаты бенчмарка
- **`benchmark_results/*_analysis.json`**: Детальный анализ результатов
- **`benchmark_results/*_metrics_table.tex`**: Таблицы метрик в LaTeX формате

## Разработка и тестирование

### Запуск тестов

```bash
cd backend
pytest
```

### Структура тестов

- `backend/tests/` - Тесты для backend API
- Тесты покрывают основные функции OpenRouter клиента и обработки изображений

## Лицензия

Этот проект предоставляется для исследовательских целей.

## Контакты

- GitHub: [Karifannaa](https://github.com/Karifannaa)
- Email: ra.khrulev@gmail.com
